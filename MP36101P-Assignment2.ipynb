{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will import all required libraries. They are a lot but we will try a lot of algorithm so it makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import time\n",
    "import numpy\n",
    "import os.path\n",
    "import urllib.request\n",
    "import sklearn.datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier, RidgeClassifier, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data set is not present in the filesystem, we download it and load it to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir('data/'):\n",
    "    os.makedirs('data/')\n",
    "\n",
    "if not os.path.isdir('data/txt_sentoken'):\n",
    "    if not os.path.isfile('data/review_polarity.tar.gz'):\n",
    "        urllib.request.urlretrieve('http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz', filename='data/review_polarity.tar.gz')\n",
    "        file = tarfile.open(name='data/review_polarity.tar.gz')\n",
    "        file.extractall(path='data/')\n",
    "\n",
    "reviews_data = sklearn.datasets.load_files('data/txt_sentoken', random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will keep 20% of the data as test set and we will use the rest 80% to train and validate the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_data_train, reviews_data_test, target_train, target_test = train_test_split(reviews_data.data,\n",
    "                                                                                    reviews_data.target,\n",
    "                                                                                    test_size=0.20,\n",
    "                                                                                    random_state=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start testing out algorithms. We will begin with Naive Bayes, a good base for tf-idf variables.\n",
    "\n",
    "We are also keeping the best scoring parameters of each algorithm in a file for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   51.3s\n",
      "[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes results:\n",
      "Best score: 0.822500\n",
      "Best parameters: {'clf__alpha': 0.1, 'tfidf__ngram_range': (1, 3), 'clf__fit_prior': True}\n",
      "Time required: 142.318642 seconds\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.81      0.89      0.85       191\n",
      "        pos       0.89      0.81      0.85       209\n",
      "\n",
      "avg / total       0.85      0.85      0.85       400\n",
      "\n",
      "[[170  21]\n",
      " [ 40 169]]\n"
     ]
    }
   ],
   "source": [
    "#Create a pipeline that performes the vectorization and then execute the ML algorithm\n",
    "reviews_classifier_NB = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                                  ('clf', MultinomialNB())])\n",
    "\n",
    "#Create a dictionary of algorithm parameters to try out\n",
    "parameters_NB = {'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                 'clf__alpha': (0.00001, 0.0001, 0.001, 0.01, 0.1, 1),\n",
    "                 'clf__fit_prior': (True, False)}\n",
    "\n",
    "#Define a grid search object that will execute the pipeline for all parameter combinations\n",
    "gs_NB = GridSearchCV(reviews_classifier_NB, parameters_NB, n_jobs=-1, verbose=1)\n",
    "\n",
    "#Fit the models to the data (and count time required)\n",
    "start_time = time.time()\n",
    "gs_NB = gs_NB.fit(reviews_data_train, target_train)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "#Print results\n",
    "print(\"Multinomial Naive Bayes results:\")\n",
    "print(\"Best score: %f\" % gs_NB.best_score_)\n",
    "print(\"Best parameters: %r\" % gs_NB.best_params_)\n",
    "print(\"Time required: %f seconds\" % elapsed_time)\n",
    "\n",
    "#We will use the test data to measure the performance\n",
    "target_predicted = gs_NB.predict(reviews_data_test)\n",
    "print(metrics.classification_report(target_test, target_predicted, target_names=reviews_data.target_names))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted))\n",
    "\n",
    "#Output to file\n",
    "result_file = open('result.txt', 'a')\n",
    "print(\"Best score: %f\" % gs_NB.best_score_, file=result_file)\n",
    "print(\"Best parameters: %r\" % gs_NB.best_params_, file=result_file)\n",
    "print(elapsed_time, file=result_file)\n",
    "print(metrics.classification_report(target_test, target_predicted, target_names=reviews_data.target_names), file=result_file)\n",
    "print(metrics.confusion_matrix(target_test, target_predicted), file=result_file)\n",
    "print(\"=================================================\", file=result_file)\n",
    "result_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can proceed with some Support Vector Machine algorithms. We will start will the Linear Support Vector Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 720 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   28.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  9.9min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-208999093cd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#Fit the models to the data (and count time required)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mgs_linearSVC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgs_linearSVC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreviews_data_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/babis/git/M36101P-Assignment2/env/lib/python3.5/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    803\u001b[0m         \"\"\"\n\u001b[1;32m--> 804\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    805\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/babis/git/M36101P-Assignment2/env/lib/python3.5/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[0;32m    551\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m                                     error_score=self.error_score)\n\u001b[1;32m--> 553\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m                 for train, test in cv)\n\u001b[0;32m    555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/babis/git/M36101P-Assignment2/env/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    810\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 812\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    813\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    814\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/babis/git/M36101P-Assignment2/env/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    760\u001b[0m                         \u001b[1;31m# a working pool as they expect.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize_pool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/babis/git/M36101P-Assignment2/env/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    729\u001b[0m                 \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 731\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    732\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m                 \u001b[1;31m# Stop dispatching any new job in the async callback thread\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 602\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    603\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 293\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Create a pipeline that performes the vectorization and then execute the ML algorithm\n",
    "reviews_classifier_linearSVC = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                                         ('clf', LinearSVC())])\n",
    "\n",
    "#Create a dictionary of algorithm parameters to try out\n",
    "parameters_linearSVC = {'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "                   'clf__tol': (0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001),\n",
    "                   'clf__C': (0.8, 0.9, 1.0, 1.1, 1.2),\n",
    "                   'clf__loss': ('hinge', 'squared_hinge'),\n",
    "                   'clf__max_iter': (100, 1000, 10000),\n",
    "                   'clf__penalty': ('l2',),\n",
    "                   'clf__multi_class': ('ovr', 'crammer_singer')}\n",
    "\n",
    "#Define a grid search object that will execute the pipeline for all parameter combinations\n",
    "gs_linearSVC = GridSearchCV(reviews_classifier_linearSVC, parameters_linearSVC, n_jobs=-1, verbose=1)\n",
    "\n",
    "#Fit the models to the data (and count time required)\n",
    "start_time = time.time()\n",
    "gs_linearSVC = gs_linearSVC.fit(reviews_data_train, target_train)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "#Print results\n",
    "print(\"Linear Support Vector Machine results:\")\n",
    "print(\"Best score: %f\" % gs_linearSVC.best_score_)\n",
    "print(\"Best parameters: %r\" % gs_linearSVC.best_params_)\n",
    "print(\"Time required: %f seconds\" % elapsed_time)\n",
    "\n",
    "#We will use the test data to measure the performance\n",
    "target_predicted = gs_linearSVC.predict(reviews_data_test)\n",
    "print(metrics.classification_report(target_test, target_predicted, target_names=reviews_data.target_names))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted))\n",
    "\n",
    "#Output to file\n",
    "result_file = open('result.txt', 'a')\n",
    "print(\"Linear Support Vector Machine results:\")\n",
    "print(\"Best score: %f\" % gs_linearSVC.best_score_, file=result_file)\n",
    "print(\"Best parameters: %r\" % gs_linearSVC.best_params_, file=result_file)\n",
    "print(elapsed_time, file=result_file)\n",
    "print(metrics.classification_report(target_test, target_predicted, target_names=reviews_data.target_names), file=result_file)\n",
    "print(metrics.confusion_matrix(target_test, target_predicted), file=result_file)\n",
    "print(\"=================================================\", file=result_file)\n",
    "result_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try other kernels of the SVM. We will put different kernels in the parameter list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a pipeline that performes the vectorization and then execute the ML algorithm\n",
    "reviews_classifier_SVC = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                                   ('clf', SVC())])\n",
    "\n",
    "#Create a dictionary of algorithm parameters to try out\n",
    "parameters_SVC = {'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "                  'clf__C': (0.8, 0.9, 1.0, 1.1, 1.2),\n",
    "                  'clf__kernel': ('poly', 'rbf', 'sigmoid'),\n",
    "                  'clf__tol': (0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001),\n",
    "                  'clf__decision_function_shape': ('ovr', 'ovo')}\n",
    "\n",
    "#Define a grid search object that will execute the pipeline for all parameter combinations\n",
    "gs_SVC = GridSearchCV(reviews_classifier_SVC, parameters_SVC, n_jobs=-1, verbose=1)\n",
    "\n",
    "#Fit the models to the data (and count time required)\n",
    "start_time = time.time()\n",
    "gs_SVC = gs_SVC.fit(reviews_data_train, target_train)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "#Print results\n",
    "print(\"Support Vector Machine results:\")\n",
    "print(\"Best score: %f\" % gs_SVC.best_score_)\n",
    "print(\"Best parameters: %r\" % gs_SVC.best_params_)\n",
    "print(\"Time required: %f seconds\" % elapsed_time)\n",
    "\n",
    "#We will use the test data to measure the performance\n",
    "target_predicted = gs_SVC.predict(reviews_data_test)\n",
    "print(metrics.classification_report(target_test, target_predicted, target_names=reviews_data.target_names))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted))\n",
    "\n",
    "#Output to file\n",
    "result_file = open('result.txt', 'a')\n",
    "print(\"Support Vector Machine results:\")\n",
    "print(\"Best score: %f\" % gs_SVC.best_score_, file=result_file)\n",
    "print(\"Best parameters: %r\" % gs_SVC.best_params_, file=result_file)\n",
    "print(elapsed_time, file=result_file)\n",
    "print(metrics.classification_report(target_test, target_predicted, target_names=reviews_data.target_names), file=result_file)\n",
    "print(metrics.confusion_matrix(target_test, target_predicted), file=result_file)\n",
    "print(\"=================================================\", file=result_file)\n",
    "result_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying another family of algorithms, we proceed to the linear models.\n",
    "\n",
    "The Passive-Aggressive algorithm yields the follwing results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a pipeline that performes the vectorization and then execute the ML algorithm\n",
    "reviews_classifier_PassiveAggressive = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                                                 ('clf', PassiveAggressiveClassifier())])\n",
    "\n",
    "#Create a dictionary of algorithm parameters to try out\n",
    "parameters_PassiveAggressive = {'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "                                'clf__C': (0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2.),\n",
    "                                'clf__fit_intercept': (True, False),\n",
    "                                'clf__n_iter': (1, 2, 3, 5, 8, 13, 21),\n",
    "                                'clf__shuffle': (True, False),\n",
    "                                'clf__loss': ('hinge', 'squared_hinge'),\n",
    "                                'clf__warm_start': (True, False)}\n",
    "\n",
    "#Define a grid search object that will execute the pipeline for all parameter combinations\n",
    "gs_PassiveAggressive = GridSearchCV(reviews_classifier_PassiveAggressive, parameters_PassiveAggressive, n_jobs=-1, verbose=1)\n",
    "\n",
    "#Fit the models to the data (and count time required)\n",
    "start_time = time.time()\n",
    "gs_PassiveAggressive = gs_PassiveAggressive.fit(reviews_data_train, target_train)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "#Print results\n",
    "print(\"Passive Aggressive linear model results:\")\n",
    "print(\"Best score: %f\" % gs_PassiveAggressive.best_score_)\n",
    "print(\"Best parameters: %r\" % gs_PassiveAggressive.best_params_)\n",
    "print(\"Time required: %f seconds\" % elapsed_time)\n",
    "\n",
    "#We will use the test data to measure the performance\n",
    "target_predicted = gs_PassiveAggressive.predict(reviews_data_test)\n",
    "print(metrics.classification_report(target_test, target_predicted, target_names=reviews_data.target_names))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted))\n",
    "\n",
    "#Output to file\n",
    "result_file = open('result.txt', 'a')\n",
    "print(\"Passive Aggressive linear model results:\")\n",
    "print(\"Best score: %f\" % gs_PassiveAggressive.best_score_, file=result_file)\n",
    "print(\"Best parameters: %r\" % gs_PassiveAggressive.best_params_, file=result_file)\n",
    "print(elapsed_time, file=result_file)\n",
    "print(metrics.classification_report(target_test, target_predicted, target_names=reviews_data.target_names), file=result_file)\n",
    "print(metrics.confusion_matrix(target_test, target_predicted), file=result_file)\n",
    "print(\"=================================================\", file=result_file)\n",
    "result_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another member of the linear model is the Ridge algorithm. The results follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a pipeline that performes the vectorization and then execute the ML algorithm\n",
    "reviews_classifier_Ridge = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                                    ('clf', RidgeClassifier())])\n",
    "\n",
    "#Create a dictionary of algorithm parameters to try out\n",
    "parameters_Ridge = {'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "                    'clf__alpha': (0.00001, 0.0001, 0.001, 0.01, 0.1, 1.),\n",
    "                    'clf__fit_intercept': (True, False),\n",
    "                    'clf__normalize': (True, False),\n",
    "                    'clf__tol': (0.00001, 0.0001, 0.001, 0.01, 0.1, 1.)}\n",
    "\n",
    "#Define a grid search object that will execute the pipeline for all parameter combinations\n",
    "gs_Ridge = GridSearchCV(reviews_classifier_Ridge, parameters_Ridge, n_jobs=-1, verbose=1)\n",
    "\n",
    "#Fit the models to the data (and count time required)\n",
    "start_time = time.time()\n",
    "gs_Ridge = gs_Ridge.fit(reviews_data_train, target_train)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "#Print results\n",
    "print(\"Ridge linear model results:\")\n",
    "print(\"Best score: %f\" % gs_Ridge.best_score_)\n",
    "print(\"Best parameters: %r\" % gs_Ridge.best_params_)\n",
    "print(\"Time required: %f seconds\" % elapsed_time)\n",
    "\n",
    "#We will use the test data to measure the performance\n",
    "target_predicted = gs_Ridge.predict(reviews_data_test)\n",
    "print(metrics.classification_report(target_test, target_predicted, target_names=reviews_data.target_names))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted))\n",
    "\n",
    "#Output to file\n",
    "result_file = open('result.txt', 'a')\n",
    "print(\"Ridge linear model results:\")\n",
    "print(\"Best score: %f\" % gs_Ridge.best_score_, file=result_file)\n",
    "print(\"Best parameters: %r\" % gs_Ridge.best_params_, file=result_file)\n",
    "print(elapsed_time, file=result_file)\n",
    "print(metrics.classification_report(target_test, target_predicted, target_names=reviews_data.target_names), file=result_file)\n",
    "print(metrics.confusion_matrix(target_test, target_predicted), file=result_file)\n",
    "print(\"=================================================\", file=result_file)\n",
    "result_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another family of classifiers is the decision trees. We will give a go to such an algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a pipeline that performes the vectorization and then execute the ML algorithm\n",
    "reviews_classifier_Decision_Tree = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                                             ('clf', DecisionTreeClassifier())])\n",
    "\n",
    "#Create a dictionary of algorithm parameters to try out\n",
    "parameters_Decision_Tree = {'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "                            'clf__criterion': ('gini', 'entropy'),\n",
    "                            'clf__splitter': ('best', 'random'),\n",
    "                            'clf__max_features': ('None', 'sqrt', 'log2')}\n",
    "\n",
    "#Define a grid search object that will execute the pipeline for all parameter combinations\n",
    "gs_Decision_Tree = GridSearchCV(reviews_classifier_Decision_Tree, parameters_Decision_Tree, n_jobs=-1, verbose=1)\n",
    "\n",
    "#Fit the models to the data (and count time required)\n",
    "start_time = time.time()\n",
    "gs_Decision_Tree = gs_Decision_Tree.fit(reviews_data_train, target_train)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "#Print results\n",
    "print(\"Decision tree results:\")\n",
    "print(\"Best score: %f\" % gs_Decision_Tree.best_score_)\n",
    "print(\"Best parameters: %r\" % gs_Decision_Tree.best_params_)\n",
    "print(\"Time required: %f seconds\" % elapsed_time)\n",
    "\n",
    "#We will use the test data to measure the performance\n",
    "target_predicted = gs_Decision_Tree.predict(reviews_data_test)\n",
    "print(metrics.classification_report(target_test, target_predicted, target_names=reviews_data.target_names))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted))\n",
    "\n",
    "#Output to file\n",
    "result_file = open('result.txt', 'a')\n",
    "print(\"Decision tree results:\")\n",
    "print(\"Best score: %f\" % gs_Decision_Tree.best_score_, file=result_file)\n",
    "print(\"Best parameters: %r\" % gs_Decision_Tree.best_params_, file=result_file)\n",
    "print(elapsed_time, file=result_file)\n",
    "print(metrics.classification_report(target_test, target_predicted, target_names=reviews_data.target_names), file=result_file)\n",
    "print(metrics.confusion_matrix(target_test, target_predicted), file=result_file)\n",
    "print(\"=================================================\", file=result_file)\n",
    "result_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will try an ensemble method. We will use many random trees and assess the performance of this method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a pipeline that performes the vectorization and then execute the ML algorithm\n",
    "reviews_classifier_ExtraTrees = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                                          ('clf', ExtraTreesClassifier())])\n",
    "\n",
    "#Create a dictionary of algorithm parameters to try out\n",
    "parameters_Extra_Trees = {'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "                          'clf__n_estimators': (10, 100, 1000, 10000, 100000)}\n",
    "\n",
    "#Define a grid search object that will execute the pipeline for all parameter combinations\n",
    "gs_ExtraTrees = GridSearchCV(reviews_classifier_ExtraTrees, parameters_Extra_Trees, n_jobs=-1, verbose=1)\n",
    "\n",
    "#Fit the models to the data (and count time required)\n",
    "start_time = time.time()\n",
    "gs_ExtraTrees = gs_ExtraTrees.fit(reviews_data_train, target_train)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "#Print results\n",
    "print(\"Ensemble extra trees results:\")\n",
    "print(\"Best score: %f\" % gs_ExtraTrees.best_score_)\n",
    "print(\"Best parameters: %r\" % gs_ExtraTrees.best_params_)\n",
    "print(\"Time required: %f seconds\" % elapsed_time)\n",
    "\n",
    "#We will use the test data to measure the performance\n",
    "target_predicted = gs_ExtraTrees.predict(reviews_data_test)\n",
    "print(metrics.classification_report(target_test, target_predicted, target_names=reviews_data.target_names))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted))\n",
    "\n",
    "#Output to file\n",
    "result_file = open('result.txt', 'a')\n",
    "print(\"Ensemble extra trees results:\")\n",
    "print(\"Best score: %f\" % gs_ExtraTrees.best_score_, file=result_file)\n",
    "print(\"Best parameters: %r\" % gs_ExtraTrees.best_params_, file=result_file)\n",
    "print(elapsed_time, file=result_file)\n",
    "print(metrics.classification_report(target_test, target_predicted, target_names=reviews_data.target_names), file=result_file)\n",
    "print(metrics.confusion_matrix(target_test, target_predicted), file=result_file)\n",
    "print(\"=================================================\", file=result_file)\n",
    "result_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
